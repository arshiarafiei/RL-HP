{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0: Initial State: 0, AP: a\n",
      "Agent 1: Initial State: 0, AP: a\n",
      "[-2, -1]\n",
      "[-2, -inf]\n",
      "-1\n",
      "[-2, -1, 0]\n",
      "[-2, -inf, -inf]\n",
      "0\n",
      "[-2, -1, 0, 1]\n",
      "[-2, -inf, -inf, -inf]\n",
      "1\n",
      "[0, 2, 3, 4]\n",
      "[0, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class SimpleMDPEnv:\n",
    "    def __init__(self):\n",
    "        # Define the state space and action space\n",
    "        self.state_space = [0, 1, 2, 3, 4, 5]  #  states\n",
    "        self.action_space = [0, 1]  # actions\n",
    "        self.initial_state = self.state_space[0]\n",
    "        self.state = self.state_space[0] \n",
    "        self.transitions = [\n",
    "            (0, 0, 2),\n",
    "            (2, 0, 3),\n",
    "            (2, 1, 2),\n",
    "            (3, 0, 4),\n",
    "            (3, 1, 3),\n",
    "            (0, 1, 1),\n",
    "            (1, 1, 5),\n",
    "            (1, 0, 1),\n",
    "            (5, 0, 5),\n",
    "            (5, 1, 5)            \n",
    "        ]\n",
    "        self.atomic_propositions = {0: \"a\", 1: \"a\", 2: \"a\", 3: \"a\", 4: \"b\", 5: \"c\"}  \n",
    "\n",
    "    def reset(self):\n",
    "        return self.initial_state\n",
    "\n",
    "    def transition_function(self, current_state, action):\n",
    "        \n",
    "        for (cur_state, act, next_state) in self.transitions:\n",
    "            if cur_state == current_state and act == action:\n",
    "                return next_state\n",
    "        raise ValueError(f\"Invalid transition for state {current_state} and action {action}.\")\n",
    "\n",
    "\n",
    "    def step(self, action, target):\n",
    "        \n",
    "        if action not in self.action_space:\n",
    "            raise ValueError(f\"Invalid action: {action}. Valid actions: {self.action_space}\")\n",
    "\n",
    "        # Transition\n",
    "        next_state = self.transition_function(self.state, action)\n",
    "\n",
    "        \n",
    "\n",
    "        # terminal condition\n",
    "        done = self.atomic_propositions[next_state] == target\n",
    "\n",
    "        self.state = next_state\n",
    "        return next_state , done\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        \n",
    "        print(f\"Current State: {self.state}, Atomic Proposition: {self.atomic_propositions[self.state]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def distance_to_target(self, start_state, target_ap):\n",
    "        \n",
    "        visited = set()\n",
    "        queue = [(start_state, 0)]  # (current_state, distance)\n",
    "\n",
    "        while queue:\n",
    "            current_state, distance = queue.pop(0)\n",
    "\n",
    "            if self.atomic_propositions[current_state] == target_ap:\n",
    "                return distance\n",
    "\n",
    "            visited.add(current_state)\n",
    "\n",
    "            for (cur_state, act, next_state) in self.transitions:\n",
    "                if cur_state == current_state and next_state not in visited:\n",
    "                    queue.append((next_state, distance + 1))\n",
    "\n",
    "        return math.inf\n",
    "\n",
    "\n",
    "\n",
    "def reward(tr, tr1, env, env1, step):\n",
    "    def calculate_phi(tr_a, tr_b, env_a, env_b, target_a, target_b):\n",
    "        temp_values = []\n",
    "\n",
    "        for i in range(max(len(tr_a), len(tr_b))):\n",
    "            if i >= len(tr_b):  # When tr_b is exhausted\n",
    "                temp_values.append(1 - env_a.distance_to_target(tr_a[i], target_a))\n",
    "                continue\n",
    "\n",
    "            if i >= len(tr_a):  \n",
    "                temp_values.append(1 - env_b.distance_to_target(tr_b[i], target_b))\n",
    "                continue\n",
    "\n",
    "            \n",
    "            temp = min(\n",
    "                1 - env_a.distance_to_target(tr_a[i], target_a),\n",
    "                1 - env_b.distance_to_target(tr_b[i], target_b)\n",
    "            )\n",
    "            temp_values.append(temp)\n",
    "\n",
    "        print(temp_values)\n",
    "        return max(temp_values)\n",
    "\n",
    "    \n",
    "    phi1 = calculate_phi(tr, tr1, env, env1, \"b\", \"c\")\n",
    "    phi2 = calculate_phi(tr, tr1, env, env1, \"c\", \"b\")\n",
    "\n",
    "    \n",
    "    return max(phi1, phi2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    env = SimpleMDPEnv()\n",
    "    env1 = SimpleMDPEnv()\n",
    "\n",
    "    trajectory  = list()\n",
    "\n",
    "    trajectory1 = list()\n",
    "\n",
    "    \n",
    "    \n",
    "    state = env.reset()\n",
    "    state1 = env1.reset()\n",
    "\n",
    "\n",
    "    trajectory.append(state)\n",
    "    trajectory1.append(state1)\n",
    "\n",
    "    print(f\"Agent 0: Initial State: {state}, AP: {env.atomic_propositions[state]}\")\n",
    "    print(f\"Agent 1: Initial State: {state1}, AP: {env1.atomic_propositions[state1]}\")\n",
    "\n",
    "\n",
    "    done = False\n",
    "    done1 = False \n",
    "\n",
    "\n",
    "    total_reward = 0\n",
    "    total_reward1 = 0\n",
    "\n",
    "    \n",
    "\n",
    "    step = 0\n",
    "\n",
    "    while (done == False or done1 == False):\n",
    "        # action = np.random.choice(env.action_space)\n",
    "        # action1 = np.random.choice(env1.action_space)\n",
    "\n",
    "        action = 0\n",
    "        action1 = 1\n",
    "\n",
    "        if done == False:\n",
    "            \n",
    "            next_state, done = env.step(action,\"b\")\n",
    "            trajectory.append(next_state)\n",
    "\n",
    "            # print(\"agent 0\")\n",
    "            # env.render()\n",
    "\n",
    "        if done1 == False:\n",
    "            next_state1, done1 = env1.step(action1,\"c\")\n",
    "\n",
    "            trajectory1.append(next_state1)\n",
    "            # print(\"agent 1\")\n",
    "            # env1.render()\n",
    "\n",
    "        \n",
    "        print(reward(trajectory, trajectory1, env, env1, step))\n",
    "            \n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(trajectory)\n",
    "    print(trajectory1)\n",
    "\n",
    "    # print(f\"Reward agent 0: {total_reward}\")\n",
    "    # print(f\"Reward agent 1: {total_reward1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-inf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "class SimpleMDPEnv:\n",
    "    def __init__(self):\n",
    "        # Define the state space and action space\n",
    "        self.state_space = [0, 1, 2, 3, 4, 5]  #  states\n",
    "        self.action_space = [0, 1]  # actions\n",
    "        self.initial_state = self.state_space[0]\n",
    "        self.state = self.state_space[0] \n",
    "        self.transitions = [\n",
    "            (0, 0, 2),\n",
    "            (2, 0, 3),\n",
    "            (2, 1, 2),\n",
    "            (3, 0, 4),\n",
    "            (3, 1, 3),\n",
    "            (0, 1, 1),\n",
    "            (1, 1, 5),\n",
    "            (1, 0, 1),\n",
    "            (5, 0, 5),\n",
    "            (5, 1, 5)            \n",
    "        ]\n",
    "        self.atomic_propositions = {0: \"a\", 1: \"a\", 2: \"a\", 3: \"a\", 4: \"b\", 5: \"c\"}  \n",
    "\n",
    "    def reset(self):\n",
    "        return self.initial_state\n",
    "\n",
    "    def transition_function(self, current_state, action):\n",
    "        \n",
    "        for (cur_state, act, next_state) in self.transitions:\n",
    "            if cur_state == current_state and act == action:\n",
    "                return next_state\n",
    "        raise ValueError(f\"Invalid transition for state {current_state} and action {action}.\")\n",
    "\n",
    "\n",
    "    def step(self, action, target):\n",
    "        \n",
    "        if action not in self.action_space:\n",
    "            raise ValueError(f\"Invalid action: {action}. Valid actions: {self.action_space}\")\n",
    "\n",
    "        # Transition\n",
    "        next_state = self.transition_function(self.state, action)\n",
    "\n",
    "        \n",
    "\n",
    "        # terminal condition\n",
    "        done = self.atomic_propositions[next_state] == target\n",
    "\n",
    "        self.state = next_state\n",
    "        return next_state , done\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        \n",
    "        print(f\"Current State: {self.state}, Atomic Proposition: {self.atomic_propositions[self.state]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def distance_to_target(self, start_state, target_ap):\n",
    "        \n",
    "        visited = set()\n",
    "        queue = [(start_state, 0)]  # (current_state, distance)\n",
    "\n",
    "        while queue:\n",
    "            current_state, distance = queue.pop(0)\n",
    "\n",
    "            if self.atomic_propositions[current_state] == target_ap:\n",
    "                return distance\n",
    "\n",
    "            visited.add(current_state)\n",
    "\n",
    "            for (cur_state, act, next_state) in self.transitions:\n",
    "                if cur_state == current_state and next_state not in visited:\n",
    "                    queue.append((next_state, distance + 1))\n",
    "\n",
    "        return math.inf\n",
    "\n",
    "\n",
    "\n",
    "def reward(tr, tr1, env, env1, step):\n",
    "    def calculate_phi(tr_a, tr_b, env_a, env_b, target_a, target_b):\n",
    "        temp_values = []\n",
    "\n",
    "        for i in range(max(len(tr_a), len(tr_b))):\n",
    "            if i >= len(tr_b):  # When tr_b is exhausted\n",
    "                temp_values.append(1 - env_a.distance_to_target(tr_a[i], target_a))\n",
    "                continue\n",
    "\n",
    "            if i >= len(tr_a):  \n",
    "                temp_values.append(1 - env_b.distance_to_target(tr_b[i], target_b))\n",
    "                continue\n",
    "\n",
    "            \n",
    "            temp = min(\n",
    "                1 - env_a.distance_to_target(tr_a[i], target_a),\n",
    "                1 - env_b.distance_to_target(tr_b[i], target_b)\n",
    "            )\n",
    "            temp_values.append(temp)\n",
    "\n",
    "        print(temp_values)\n",
    "        return max(temp_values)\n",
    "\n",
    "    \n",
    "    phi1 = calculate_phi(tr, tr1, env, env1, \"b\", \"c\")\n",
    "    phi2 = calculate_phi(tr, tr1, env, env1, \"c\", \"b\")\n",
    "\n",
    "    \n",
    "    return max(phi1, phi2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "class MultiAgentDQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size * 2  # Combined state size for two agents\n",
    "        self.action_size = action_size * 2  # Combined action size for two agents\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return [random.randrange(2), random.randrange(2)]  # Two random actions\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return [np.argmax(act_values[0][:2]), np.argmax(act_values[0][2:])]  # Actions for both agents\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action[0]] = target\n",
    "            target_f[0][action[1] + 2] = target  # Offset for second agent actions\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env1 = SimpleMDPEnv()\n",
    "    env2 = SimpleMDPEnv()\n",
    "\n",
    "    state_size = 1  # Single integer representing the state\n",
    "    action_size = len(env1.action_space)\n",
    "\n",
    "    agent = MultiAgentDQN(state_size, action_size)\n",
    "\n",
    "    episodes = 1000\n",
    "    batch_size = 32\n",
    "\n",
    "    target_proposition1 = input(\"Enter the target atomic proposition for Environment 1 (e.g., 'e'): \")\n",
    "    target_proposition2 = input(\"Enter the target atomic proposition for Environment 2 (e.g., 'e'): \")\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state1 = env1.reset()\n",
    "        state2 = env2.reset()\n",
    "\n",
    "        combined_state = np.reshape([state1, state2], [1, state_size * 2])\n",
    "\n",
    "        total_reward1 = 0\n",
    "        total_reward2 = 0\n",
    "\n",
    "        for time in range(500):\n",
    "            # Multi-agent action\n",
    "            actions = agent.act(combined_state)\n",
    "            action1, action2 = actions[0], actions[1]\n",
    "\n",
    "            # Agent 1 environment step\n",
    "            next_state1, reward1, done1 = env1.step(action1, target_proposition1)\n",
    "            total_reward1 += reward1\n",
    "\n",
    "            # Agent 2 environment step\n",
    "            next_state2, reward2, done2 = env2.step(action2, target_proposition2)\n",
    "            total_reward2 += reward2\n",
    "\n",
    "            combined_next_state = np.reshape([next_state1, next_state2], [1, state_size * 2])\n",
    "\n",
    "            agent.remember(combined_state, actions, reward1 + reward2, combined_next_state, done1 and done2)\n",
    "            combined_state = combined_next_state\n",
    "\n",
    "            if done1 and done2:\n",
    "                print(f\"Episode {e+1}/{episodes} - Reward1: {total_reward1}, Reward2: {total_reward2}, Epsilon: {agent.epsilon:.2f}\")\n",
    "                break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "\n",
    "class SimpleMDPEnv:\n",
    "    def __init__(self):\n",
    "        # Define the state space and action space\n",
    "        self.state_space = [0, 1, 2, 3, 4]  # Example states\n",
    "        self.action_space = [0, 1]  # Example actions: 0 and 1\n",
    "        self.state = None  # Current state\n",
    "        self.atomic_propositions = {0: \"a\", 1: \"b\", 2: \"c\", 3: \"d\", 4: \"e\"}  # Atomic propositions for each state\n",
    "\n",
    "        # Transition array: (current_state, action, next_state)\n",
    "        self.transitions = [\n",
    "            (0, 0, 4), (0, 1, 1),\n",
    "            (1, 0, 0), (1, 1, 2),\n",
    "            (2, 0, 1), (2, 1, 3),\n",
    "            (3, 0, 2), (3, 1, 4),\n",
    "            (4, 0, 3), (4, 1, 0)\n",
    "        ]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "        self.state = np.random.choice(self.state_space)  # Random initial state\n",
    "        return self.state\n",
    "\n",
    "    def transition_function(self, current_state, action):\n",
    "        \"\"\"Encodes the transition logic between states using a transition array.\n",
    "\n",
    "        Args:\n",
    "            current_state (int): The current state.\n",
    "            action (int): The action to take.\n",
    "\n",
    "        Returns:\n",
    "            next_state (int): The next state after taking the action.\n",
    "        \"\"\"\n",
    "        for (cur_state, act, next_state) in self.transitions:\n",
    "            if cur_state == current_state and act == action:\n",
    "                return next_state\n",
    "        raise ValueError(f\"Invalid transition for state {current_state} and action {action}.\")\n",
    "\n",
    "    def step(self, action, target_proposition):\n",
    "        \"\"\"Takes an action and returns the next state, reward, and done flag.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action to take (must be in the action space).\n",
    "            target_proposition (str): The atomic proposition that marks the terminal state.\n",
    "\n",
    "        Returns:\n",
    "            next_state (int): The next state after taking the action.\n",
    "            reward (float): The reward for taking the action.\n",
    "            done (bool): Whether the episode is finished.\n",
    "        \"\"\"\n",
    "        if action not in self.action_space:\n",
    "            raise ValueError(f\"Invalid action: {action}. Valid actions: {self.action_space}\")\n",
    "\n",
    "        # Use the transition function to determine the next state\n",
    "        next_state = self.transition_function(self.state, action)\n",
    "\n",
    "        # Example reward logic (can be customized)\n",
    "        reward = 1 if self.atomic_propositions[next_state] == target_proposition else -0.1\n",
    "\n",
    "        # Terminal condition based on atomic proposition\n",
    "        done = self.atomic_propositions[next_state] == target_proposition\n",
    "\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Renders the current state.\"\"\"\n",
    "        print(f\"Current State: {self.state}, Atomic Proposition: {self.atomic_propositions[self.state]}\")\n",
    "\n",
    "# DQN Agent\n",
    "class MultiAgentDQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size * 2  # Combined state size for two agents\n",
    "        self.action_size = action_size * 2  # Combined action size for two agents\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return [random.randrange(2), random.randrange(2)]  # Two random actions\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return [np.argmax(act_values[0][:2]), np.argmax(act_values[0][2:])]  # Actions for both agents\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action[0]] = target\n",
    "            target_f[0][action[1] + 2] = target  # Offset for second agent actions\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    env1 = SimpleMDPEnv()\n",
    "    env2 = SimpleMDPEnv()\n",
    "\n",
    "    state_size = 1  # Single integer representing the state\n",
    "    action_size = len(env1.action_space)\n",
    "\n",
    "    agent = MultiAgentDQN(state_size, action_size)\n",
    "\n",
    "    episodes = 1000\n",
    "    batch_size = 32\n",
    "\n",
    "    target_proposition1 = input(\"Enter the target atomic proposition for Environment 1 (e.g., 'e'): \")\n",
    "    target_proposition2 = input(\"Enter the target atomic proposition for Environment 2 (e.g., 'e'): \")\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state1 = env1.reset()\n",
    "        state2 = env2.reset()\n",
    "\n",
    "        combined_state = np.reshape([state1, state2], [1, state_size * 2])\n",
    "\n",
    "        total_reward1 = 0\n",
    "        total_reward2 = 0\n",
    "\n",
    "        for time in range(500):\n",
    "            # Multi-agent action\n",
    "            actions = agent.act(combined_state)\n",
    "            action1, action2 = actions[0], actions[1]\n",
    "\n",
    "            # Agent 1 environment step\n",
    "            next_state1, reward1, done1 = env1.step(action1, target_proposition1)\n",
    "            total_reward1 += reward1\n",
    "\n",
    "            # Agent 2 environment step\n",
    "            next_state2, reward2, done2 = env2.step(action2, target_proposition2)\n",
    "            total_reward2 += reward2\n",
    "\n",
    "            combined_next_state = np.reshape([next_state1, next_state2], [1, state_size * 2])\n",
    "\n",
    "            agent.remember(combined_state, actions, reward1 + reward2, combined_next_state, done1 and done2)\n",
    "            combined_state = combined_next_state\n",
    "\n",
    "            if done1 and done2:\n",
    "                print(f\"Episode {e+1}/{episodes} - Reward1: {total_reward1}, Reward2: {total_reward2}, Epsilon: {agent.epsilon:.2f}\")\n",
    "                break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
