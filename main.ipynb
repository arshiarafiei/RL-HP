{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0: Initial State: 0, AP: a\n",
      "Agent 1: Initial State: 0, AP: a\n",
      "[-2, -1]\n",
      "[-2, -inf]\n",
      "-1\n",
      "[-2, -1, 0]\n",
      "[-2, -inf, -inf]\n",
      "0\n",
      "[-2, -1, 0, 1]\n",
      "[-2, -inf, -inf, -inf]\n",
      "1\n",
      "[0, 2, 3, 4]\n",
      "[0, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class SimpleMDPEnv:\n",
    "    def __init__(self):\n",
    "        # Define the state space and action space\n",
    "        self.state_space = [0, 1, 2, 3, 4, 5]  #  states\n",
    "        self.action_space = [0, 1]  # actions\n",
    "        self.initial_state = self.state_space[0]\n",
    "        self.state = self.state_space[0] \n",
    "        self.transitions = [\n",
    "            (0, 0, 2),\n",
    "            (2, 0, 3),\n",
    "            (2, 1, 2),\n",
    "            (3, 0, 4),\n",
    "            (3, 1, 3),\n",
    "            (0, 1, 1),\n",
    "            (1, 1, 5),\n",
    "            (1, 0, 1),\n",
    "            (5, 0, 5),\n",
    "            (5, 1, 5)            \n",
    "        ]\n",
    "        self.atomic_propositions = {0: \"a\", 1: \"a\", 2: \"a\", 3: \"a\", 4: \"b\", 5: \"c\"}  \n",
    "\n",
    "    def reset(self):\n",
    "        return self.initial_state\n",
    "\n",
    "    def transition_function(self, current_state, action):\n",
    "        \n",
    "        for (cur_state, act, next_state) in self.transitions:\n",
    "            if cur_state == current_state and act == action:\n",
    "                return next_state\n",
    "        raise ValueError(f\"Invalid transition for state {current_state} and action {action}.\")\n",
    "\n",
    "\n",
    "    def step(self, action, target):\n",
    "        \n",
    "        if action not in self.action_space:\n",
    "            raise ValueError(f\"Invalid action: {action}. Valid actions: {self.action_space}\")\n",
    "\n",
    "        # Transition\n",
    "        next_state = self.transition_function(self.state, action)\n",
    "\n",
    "        \n",
    "\n",
    "        # terminal condition\n",
    "        done = self.atomic_propositions[next_state] == target\n",
    "\n",
    "        self.state = next_state\n",
    "        return next_state , done\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        \n",
    "        print(f\"Current State: {self.state}, Atomic Proposition: {self.atomic_propositions[self.state]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def distance_to_target(self, start_state, target_ap):\n",
    "        \n",
    "        visited = set()\n",
    "        queue = [(start_state, 0)]  # (current_state, distance)\n",
    "\n",
    "        while queue:\n",
    "            current_state, distance = queue.pop(0)\n",
    "\n",
    "            if self.atomic_propositions[current_state] == target_ap:\n",
    "                return distance\n",
    "\n",
    "            visited.add(current_state)\n",
    "\n",
    "            for (cur_state, act, next_state) in self.transitions:\n",
    "                if cur_state == current_state and next_state not in visited:\n",
    "                    queue.append((next_state, distance + 1))\n",
    "\n",
    "        return math.inf\n",
    "\n",
    "\n",
    "\n",
    "def reward(tr, tr1, env, env1, step):\n",
    "    def calculate_phi(tr_a, tr_b, env_a, env_b, target_a, target_b):\n",
    "        temp_values = []\n",
    "\n",
    "        for i in range(max(len(tr_a), len(tr_b))):\n",
    "            if i >= len(tr_b):  # When tr_b is exhausted\n",
    "                temp_values.append(1 - env_a.distance_to_target(tr_a[i], target_a))\n",
    "                continue\n",
    "\n",
    "            if i >= len(tr_a):  \n",
    "                temp_values.append(1 - env_b.distance_to_target(tr_b[i], target_b))\n",
    "                continue\n",
    "\n",
    "            \n",
    "            temp = min(\n",
    "                1 - env_a.distance_to_target(tr_a[i], target_a),\n",
    "                1 - env_b.distance_to_target(tr_b[i], target_b)\n",
    "            )\n",
    "            temp_values.append(temp)\n",
    "\n",
    "        print(temp_values)\n",
    "        return max(temp_values)\n",
    "\n",
    "    \n",
    "    phi1 = calculate_phi(tr, tr1, env, env1, \"b\", \"c\")\n",
    "    phi2 = calculate_phi(tr, tr1, env, env1, \"c\", \"b\")\n",
    "\n",
    "    \n",
    "    return max(phi1, phi2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    env = SimpleMDPEnv()\n",
    "    env1 = SimpleMDPEnv()\n",
    "\n",
    "    trajectory  = list()\n",
    "\n",
    "    trajectory1 = list()\n",
    "\n",
    "    \n",
    "    \n",
    "    state = env.reset()\n",
    "    state1 = env1.reset()\n",
    "\n",
    "\n",
    "    trajectory.append(state)\n",
    "    trajectory1.append(state1)\n",
    "\n",
    "    print(f\"Agent 0: Initial State: {state}, AP: {env.atomic_propositions[state]}\")\n",
    "    print(f\"Agent 1: Initial State: {state1}, AP: {env1.atomic_propositions[state1]}\")\n",
    "\n",
    "\n",
    "    done = False\n",
    "    done1 = False \n",
    "\n",
    "\n",
    "    total_reward = 0\n",
    "    total_reward1 = 0\n",
    "\n",
    "    \n",
    "\n",
    "    step = 0\n",
    "\n",
    "    while (done == False or done1 == False):\n",
    "        # action = np.random.choice(env.action_space)\n",
    "        # action1 = np.random.choice(env1.action_space)\n",
    "\n",
    "        action = 0\n",
    "        action1 = 1\n",
    "\n",
    "        if done == False:\n",
    "            \n",
    "            next_state, done = env.step(action,\"b\")\n",
    "            trajectory.append(next_state)\n",
    "\n",
    "            # print(\"agent 0\")\n",
    "            # env.render()\n",
    "\n",
    "        if done1 == False:\n",
    "            next_state1, done1 = env1.step(action1,\"c\")\n",
    "\n",
    "            trajectory1.append(next_state1)\n",
    "            # print(\"agent 1\")\n",
    "            # env1.render()\n",
    "\n",
    "        \n",
    "        print(reward(trajectory, trajectory1, env, env1, step))\n",
    "            \n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(trajectory)\n",
    "    print(trajectory1)\n",
    "\n",
    "    # print(f\"Reward agent 0: {total_reward}\")\n",
    "    # print(f\"Reward agent 1: {total_reward1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "class SimpleMDPEnv:\n",
    "    def __init__(self):\n",
    "        # Define the state space and action space\n",
    "        self.state_space = [0, 1, 2, 3, 4, 5]  #  states\n",
    "        self.action_space = [0, 1]  # actions\n",
    "        self.initial_state = self.state_space[0]\n",
    "        self.state = self.state_space[0] \n",
    "        self.transitions = [\n",
    "            (0, 0, 2),\n",
    "            (2, 0, 3),\n",
    "            (2, 1, 2),\n",
    "            (3, 0, 4),\n",
    "            (3, 1, 3),\n",
    "            (0, 1, 1),\n",
    "            (1, 1, 5),\n",
    "            (1, 0, 1),\n",
    "            (5, 0, 5),\n",
    "            (5, 1, 5)            \n",
    "        ]\n",
    "        self.atomic_propositions = {0: \"a\", 1: \"a\", 2: \"a\", 3: \"a\", 4: \"b\", 5: \"c\"}  \n",
    "\n",
    "    def reset(self):\n",
    "        return self.initial_state\n",
    "\n",
    "    def transition_function(self, current_state, action):\n",
    "        \n",
    "        for (cur_state, act, next_state) in self.transitions:\n",
    "            if cur_state == current_state and act == action:\n",
    "                return next_state\n",
    "        raise ValueError(f\"Invalid transition for state {current_state} and action {action}.\")\n",
    "\n",
    "\n",
    "    def step(self, action, target):\n",
    "        \n",
    "        if action not in self.action_space:\n",
    "            raise ValueError(f\"Invalid action: {action}. Valid actions: {self.action_space}\")\n",
    "\n",
    "        # Transition\n",
    "        next_state = self.transition_function(self.state, action)\n",
    "\n",
    "        \n",
    "\n",
    "        # terminal condition\n",
    "        done = self.atomic_propositions[next_state] == target\n",
    "\n",
    "        self.state = next_state\n",
    "        return next_state , done\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        \n",
    "        print(f\"Current State: {self.state}, Atomic Proposition: {self.atomic_propositions[self.state]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def distance_to_target(self, start_state, target_ap):\n",
    "        \n",
    "        visited = set()\n",
    "        queue = [(start_state, 0)]  # (current_state, distance)\n",
    "\n",
    "        while queue:\n",
    "            current_state, distance = queue.pop(0)\n",
    "\n",
    "            if self.atomic_propositions[current_state] == target_ap:\n",
    "                return distance\n",
    "\n",
    "            visited.add(current_state)\n",
    "\n",
    "            for (cur_state, act, next_state) in self.transitions:\n",
    "                if cur_state == current_state and next_state not in visited:\n",
    "                    queue.append((next_state, distance + 1))\n",
    "\n",
    "        return math.inf\n",
    "\n",
    "\n",
    "\n",
    "def reward(tr, tr1, env, env1, step):\n",
    "    def calculate_phi(tr_a, tr_b, env_a, env_b, target_a, target_b):\n",
    "        temp_values = []\n",
    "\n",
    "        for i in range(max(len(tr_a), len(tr_b))):\n",
    "            if i >= len(tr_b):  # When tr_b is exhausted\n",
    "                temp_values.append(1 - env_a.distance_to_target(tr_a[i], target_a))\n",
    "                continue\n",
    "\n",
    "            if i >= len(tr_a):  \n",
    "                temp_values.append(1 - env_b.distance_to_target(tr_b[i], target_b))\n",
    "                continue\n",
    "\n",
    "            \n",
    "            temp = min(\n",
    "                1 - env_a.distance_to_target(tr_a[i], target_a),\n",
    "                1 - env_b.distance_to_target(tr_b[i], target_b)\n",
    "            )\n",
    "            temp_values.append(temp)\n",
    "\n",
    "        print(temp_values)\n",
    "        return max(temp_values)\n",
    "\n",
    "    \n",
    "    phi1 = calculate_phi(tr, tr1, env, env1, \"b\", \"c\")\n",
    "    phi2 = calculate_phi(tr, tr1, env, env1, \"c\", \"b\")\n",
    "\n",
    "    \n",
    "    return max(phi1, phi2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "class MultiAgentDQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size * 2  # Combined state size for two agents\n",
    "        self.action_size = action_size * 2  # Combined action size for two agents\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return [random.randrange(2), random.randrange(2)]  # Two random actions\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return [np.argmax(act_values[0][:2]), np.argmax(act_values[0][2:])]  # Actions for both agents\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action[0]] = target\n",
    "            target_f[0][action[1] + 2] = target  # Offset for second agent actions\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env1 = SimpleMDPEnv()\n",
    "    env2 = SimpleMDPEnv()\n",
    "\n",
    "    state_size = 1  # Single integer representing the state\n",
    "    action_size = len(env1.action_space)\n",
    "\n",
    "    agent = MultiAgentDQN(state_size, action_size)\n",
    "\n",
    "    episodes = 1000\n",
    "    batch_size = 32\n",
    "\n",
    "    target_proposition1 = input(\"Enter the target atomic proposition for Environment 1 (e.g., 'e'): \")\n",
    "    target_proposition2 = input(\"Enter the target atomic proposition for Environment 2 (e.g., 'e'): \")\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state1 = env1.reset()\n",
    "        state2 = env2.reset()\n",
    "\n",
    "        combined_state = np.reshape([state1, state2], [1, state_size * 2])\n",
    "\n",
    "        total_reward1 = 0\n",
    "        total_reward2 = 0\n",
    "\n",
    "        for time in range(500):\n",
    "            # Multi-agent action\n",
    "            actions = agent.act(combined_state)\n",
    "            action1, action2 = actions[0], actions[1]\n",
    "\n",
    "            # Agent 1 environment step\n",
    "            next_state1, reward1, done1 = env1.step(action1, target_proposition1)\n",
    "            total_reward1 += reward1\n",
    "\n",
    "            # Agent 2 environment step\n",
    "            next_state2, reward2, done2 = env2.step(action2, target_proposition2)\n",
    "            total_reward2 += reward2\n",
    "\n",
    "            combined_next_state = np.reshape([next_state1, next_state2], [1, state_size * 2])\n",
    "\n",
    "            agent.remember(combined_state, actions, reward1 + reward2, combined_next_state, done1 and done2)\n",
    "            combined_state = combined_next_state\n",
    "\n",
    "            if done1 and done2:\n",
    "                print(f\"Episode {e+1}/{episodes} - Reward1: {total_reward1}, Reward2: {total_reward2}, Epsilon: {agent.epsilon:.2f}\")\n",
    "                break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/__init__.py:467\u001b[0m\n\u001b[1;32m    465\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_keras.src.optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    466\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.src.optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[1;32m    469\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DO NOT EDIT. Generated by api_gen.sh\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTypePolicy\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatDTypePolicy\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/api/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/api/activations/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/activations/__init__.py:30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tanh_shrink\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m object_registration\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n\u001b[1;32m     33\u001b[0m ALL_OBJECTS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     34\u001b[0m     relu,\n\u001b[1;32m     35\u001b[0m     leaky_relu,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     log_sigmoid,\n\u001b[1;32m     60\u001b[0m }\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/saving/__init__.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobject_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_registered_object\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobject_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_keras_serializable\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize_keras_object\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize_keras_object\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/saving/saving_api.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m legacy_h5_format\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m file_utils\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/legacy/saving/legacy_h5_format.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m json_utils\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_options\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_utils\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m object_registration\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/legacy/saving/saving_utils.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics \u001b[38;5;28;01mas\u001b[39;00m metrics_module\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizers\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/models/functional.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_utils\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization \u001b[38;5;28;01mas\u001b[39;00m legacy_serialization\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _build_map\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/models/model.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariable_mapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_saveable_variables\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_api\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trainer \u001b[38;5;28;01mas\u001b[39;00m base_trainer\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary_utils\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m traceback_utils\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/trainer.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompileLoss\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompileMetrics\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_adapter_utils\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m python_utils\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m traceback_utils\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribution_lib\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_data_adapter\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_adapter\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_dataset_adapter\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_slicing\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_adapter_utils\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataAdapter\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/array_slicing.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/__init__.py:26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m         is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     _module \u001b[38;5;241m=\u001b[39m _err\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/compat/__init__.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     pa_version_under10p1,\n\u001b[1;32m     29\u001b[0m     pa_version_under11p0,\n\u001b[1;32m     30\u001b[0m     pa_version_under13p0,\n\u001b[1;32m     31\u001b[0m     pa_version_under14p0,\n\u001b[1;32m     32\u001b[0m     pa_version_under14p1,\n\u001b[1;32m     33\u001b[0m     pa_version_under16p0,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/compat/pyarrow.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     _palv \u001b[38;5;241m=\u001b[39m Version(Version(pa\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version)\n\u001b[1;32m     11\u001b[0m     pa_version_under10p1 \u001b[38;5;241m=\u001b[39m _palv \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/__init__.py:65\u001b[0m\n\u001b[1;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[1;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[1;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:645\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "\n",
    "class SimpleMDPEnv:\n",
    "    def __init__(self):\n",
    "        # Define the state space and action space\n",
    "        self.state_space = [0, 1, 2, 3, 4, 5]  #  states\n",
    "        self.action_space = [0, 1]  # actions\n",
    "        self.initial_state = self.state_space[0]\n",
    "        self.state = self.state_space[0] \n",
    "        self.transitions = [\n",
    "            (0, 0, 2),\n",
    "            (2, 0, 3),\n",
    "            (2, 1, 2),\n",
    "            (3, 0, 4),\n",
    "            (3, 1, 3),\n",
    "            (0, 1, 1),\n",
    "            (1, 1, 5),\n",
    "            (1, 0, 1),\n",
    "            (5, 0, 5),\n",
    "            (5, 1, 5)            \n",
    "        ]\n",
    "        self.atomic_propositions = {0: \"a\", 1: \"a\", 2: \"a\", 3: \"a\", 4: \"b\", 5: \"c\"}  \n",
    "\n",
    "    def reset(self):\n",
    "        return self.initial_state\n",
    "\n",
    "    def transition_function(self, current_state, action):\n",
    "        \n",
    "        for (cur_state, act, next_state) in self.transitions:\n",
    "            if cur_state == current_state and act == action:\n",
    "                return next_state\n",
    "        raise ValueError(f\"Invalid transition for state {current_state} and action {action}.\")\n",
    "\n",
    "\n",
    "    def step(self, action, target):\n",
    "        \n",
    "        if action not in self.action_space:\n",
    "            raise ValueError(f\"Invalid action: {action}. Valid actions: {self.action_space}\")\n",
    "\n",
    "        # Transition\n",
    "        next_state = self.transition_function(self.state, action)\n",
    "\n",
    "        \n",
    "\n",
    "        # terminal condition\n",
    "        done = self.atomic_propositions[next_state] == target\n",
    "\n",
    "        self.state = next_state\n",
    "        return next_state , done\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        \n",
    "        print(f\"Current State: {self.state}, Atomic Proposition: {self.atomic_propositions[self.state]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def distance_to_target(self, start_state, target_ap):\n",
    "        \n",
    "        visited = set()\n",
    "        queue = [(start_state, 0)]  # (current_state, distance)\n",
    "\n",
    "        while queue:\n",
    "            current_state, distance = queue.pop(0)\n",
    "\n",
    "            if self.atomic_propositions[current_state] == target_ap:\n",
    "                return distance\n",
    "\n",
    "            visited.add(current_state)\n",
    "\n",
    "            for (cur_state, act, next_state) in self.transitions:\n",
    "                if cur_state == current_state and next_state not in visited:\n",
    "                    queue.append((next_state, distance + 1))\n",
    "\n",
    "        return math.inf\n",
    "\n",
    "\n",
    "\n",
    "def reward(tr, tr1, env, env1, step):\n",
    "    def calculate_phi(tr_a, tr_b, env_a, env_b, target_a, target_b):\n",
    "        temp_values = []\n",
    "\n",
    "        for i in range(max(len(tr_a), len(tr_b))):\n",
    "            if i >= len(tr_b):  # When tr_b is exhausted\n",
    "                temp_values.append(1 - env_a.distance_to_target(tr_a[i], target_a))\n",
    "                continue\n",
    "\n",
    "            if i >= len(tr_a):  \n",
    "                temp_values.append(1 - env_b.distance_to_target(tr_b[i], target_b))\n",
    "                continue\n",
    "\n",
    "            \n",
    "            temp = min(\n",
    "                1 - env_a.distance_to_target(tr_a[i], target_a),\n",
    "                1 - env_b.distance_to_target(tr_b[i], target_b)\n",
    "            )\n",
    "            temp_values.append(temp)\n",
    "\n",
    "        print(temp_values)\n",
    "        return max(temp_values)\n",
    "\n",
    "    \n",
    "    phi1 = calculate_phi(tr, tr1, env, env1, \"b\", \"c\")\n",
    "    phi2 = calculate_phi(tr, tr1, env, env1, \"c\", \"b\")\n",
    "\n",
    "    \n",
    "    return max(phi1, phi2)\n",
    "\n",
    "\n",
    "# DQN Agent\n",
    "class MultiAgentDQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size * 2  # Combined state size for two agents\n",
    "        self.action_size = action_size * 2  # Combined action size for two agents\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return [random.randrange(2), random.randrange(2)]  # Two random actions\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return [np.argmax(act_values[0][:2]), np.argmax(act_values[0][2:])]  # Actions for both agents\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action[0]] = target\n",
    "            target_f[0][action[1] + 2] = target  # Offset for second agent actions\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    env1 = SimpleMDPEnv()\n",
    "    env2 = SimpleMDPEnv()\n",
    "\n",
    "    \n",
    "\n",
    "    state_size = 1  \n",
    "    action_size = len(env1.action_space)\n",
    "\n",
    "    agent = MultiAgentDQN(state_size, action_size)\n",
    "\n",
    "    episodes = 100\n",
    "    batch_size = 32\n",
    "\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state1 = env1.reset()\n",
    "        state2 = env2.reset()\n",
    "\n",
    "        trajectory1  = list()\n",
    "\n",
    "        trajectory2 = list()\n",
    "\n",
    "        combined_state = np.reshape([state1, state2], [1, state_size * 2])\n",
    "\n",
    "        total_reward = 0\n",
    "\n",
    "        done1 = False\n",
    "        done2 = False\n",
    "\n",
    "        for time in range(500):\n",
    "            \n",
    "            actions = agent.act(combined_state)\n",
    "\n",
    "            action1, action2 = actions[0], actions[1]\n",
    "\n",
    "\n",
    "            if done == False:\n",
    "            \n",
    "                next_state1, done1 = env1.step(action,\"b\")\n",
    "                trajectory1.append(next_state1)\n",
    "\n",
    "            \n",
    "            if done1 == False:\n",
    "\n",
    "                next_state2, done2 = env2.step(action1,\"c\")\n",
    "                trajectory2.append(next_state2)\n",
    "\n",
    "\n",
    "            total_reward += reward(trajectory, trajectory1, env, env1, step)\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            combined_next_state = np.reshape([next_state1, next_state2], [1, state_size * 2])\n",
    "\n",
    "            agent.remember(combined_state, actions, total_reward, combined_next_state, done1 and done2)\n",
    "            combined_state = combined_next_state\n",
    "\n",
    "            if done1 and done2:\n",
    "                print(f\"Episode {e+1}/{episodes} - Reward{total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
    "                break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0\n",
      "0\n",
      "0 0 0\n",
      "0\n",
      "0 0 0\n",
      "0\n",
      "0 0 0\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 227\u001b[0m\n\u001b[1;32m    221\u001b[0m         trajectory1, domino1 \u001b[38;5;241m=\u001b[39m env1\u001b[38;5;241m.\u001b[39mstep(action1)\n\u001b[1;32m    225\u001b[0m     reward(trajectory, trajectory1, domino, domino1, step)\n\u001b[0;32m--> 227\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# print(f\"Reward agent 0: {total_reward}\")\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# print(f\"Reward agent 1: {total_reward1}\")\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time \n",
    "\n",
    "\n",
    "class PCPMDPEnv:\n",
    "    def __init__(self):\n",
    "        # Define the dominos context (action -> top and bottom strings)\n",
    "        self.dominos_context = {\n",
    "            1: (\"ab\", \"a\"),\n",
    "            2: (\"aba\", \"ba\"),\n",
    "            3: (\"c\", \"ba\"),\n",
    "            4: (\"bb\", \"cb\"),\n",
    "            5: (\"c\", \"bc\")\n",
    "        }  # Domino format: action -> (top string, bottom string)\n",
    "\n",
    "        self.action_space = [1, 2, 3, 4, 5]   # Actions correspond to the dominos\n",
    "        self.state_space = [1, 2, 3, 4, 5]  # States represent the labels of dominos picked\n",
    "        self.state = None  # Current state represented as the sequence of picked dominos (labels)\n",
    "        self.domino_strings = (\"\", \"\")  # Current top and bottom strings\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
    "        self.state = []  # Start with no dominos picked (empty state sequence)\n",
    "        self.domino_strings = (\"\", \"\")  # Empty top and bottom strings\n",
    "        return self.state\n",
    "\n",
    "    def transition_function(self, current_state, action):\n",
    "        \"\"\"Encodes the transition logic by appending the chosen domino to the current state.\n",
    "\n",
    "        Args:\n",
    "            current_state (list): The current state (sequence of picked dominos).\n",
    "            action (int): The action to take (pick a domino).\n",
    "\n",
    "        Returns:\n",
    "            next_state (list): The next state after taking the action.\n",
    "        \"\"\"\n",
    "        next_state = current_state + [action]  # Append the domino label to the state sequence\n",
    "        return next_state\n",
    "\n",
    "    def update_domino_strings(self, action):\n",
    "        \"\"\"Updates the top and bottom strings based on the chosen domino.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action to take (pick a domino).\n",
    "        \"\"\"\n",
    "        top_string, bottom_string = self.domino_strings\n",
    "        domino_top, domino_bottom = self.dominos_context[action]\n",
    "        self.domino_strings = (top_string + domino_top, bottom_string + domino_bottom)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes an action and returns the next state, reward, and done flag.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action to take (must be in the action space).\n",
    "\n",
    "        Returns:\n",
    "            next_state (list): The next state after taking the action.\n",
    "            reward (float): The reward for taking the action.\n",
    "            done (bool): Whether the episode is finished.\n",
    "        \"\"\"\n",
    "        if action not in self.action_space:\n",
    "            raise ValueError(f\"Invalid action: {action}. Valid actions: {self.action_space}\")\n",
    "\n",
    "        # Update the state with the domino label\n",
    "        next_state = self.transition_function(self.state, action)\n",
    "\n",
    "        # Update the domino strings (top and bottom)\n",
    "        self.update_domino_strings(action)\n",
    "\n",
    "        # Check if the top and bottom strings are equal\n",
    "        done = self.domino_strings[0] == self.domino_strings[1]\n",
    "\n",
    "        # Reward logic\n",
    "\n",
    "        next_domino = self.domino_strings\n",
    "        \n",
    "\n",
    "        self.state = next_state\n",
    "\n",
    "\n",
    "        return next_state, next_domino\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Renders the current state.\"\"\"\n",
    "        print(f\"Current State (Domino Labels): {self.state}\")\n",
    "        # print(f\"Domino Strings: Top: {self.domino_strings[0]}, Bottom: {self.domino_strings[1]}\")\n",
    "\n",
    "\n",
    "def reward(trajectory, trajectory1, domino, domino1, step):\n",
    "\n",
    "    ##############Semimatch############## \n",
    "\n",
    "    ### phi1\n",
    "\n",
    "    phi_1_list = list()\n",
    "\n",
    "    domino_1_top = domino[0] + \"#\"\n",
    "\n",
    "    domino_1_bottom = domino[1] + \"#\"\n",
    "\n",
    "\n",
    "    for index in range(min(len(domino_1_top), len(domino_1_bottom))-1):\n",
    "\n",
    "        if domino_1_top[index] ==  domino_1_bottom[index]:\n",
    "            phi_1_list.append(1) ## 1 - dist (a,a) = 1\n",
    "        else:\n",
    "            phi_1_list.append(0)  ## 1 - dist (a,b) = 0\n",
    "\n",
    "    phi_1 = min(phi_1_list)\n",
    "\n",
    "    #######phi_2\n",
    "\n",
    "    index = min(len(domino_1_top), len(domino_1_bottom)) - 1\n",
    "\n",
    "    phi_2_list = [min(-1*int(domino_1_top[index]== \"#\"), int(domino_1_bottom[index]== \"#\")), min(int(domino_1_top[index]== \"#\"), -1 * int(domino_1_bottom[index]== \"#\"))]\n",
    "\n",
    "    phi_2 = max(phi_2_list)\n",
    "\n",
    "    semimatch = max(phi_1,phi_2)\n",
    "\n",
    "    #################Match##################\n",
    "\n",
    "    domino_2_top = domino1[0] + \"#\"\n",
    "\n",
    "    domino_2_bottom = domino1[1] + \"#\"\n",
    "\n",
    "    max_length = max(len(domino_2_top), len(domino_2_bottom))\n",
    "\n",
    "    # Padding \n",
    "\n",
    "    # print(domino_2_top, domino_2_bottom)\n",
    "\n",
    "    domino_2_top = domino_2_top.ljust(max_length, '#')\n",
    "    domino_2_bottom = domino_2_bottom.ljust(max_length, '#')\n",
    "\n",
    "    # print(domino_2_top,domino_2_bottom)\n",
    "\n",
    "    match_list = list()\n",
    "\n",
    "    for index in range(min(len(domino_2_top), len(domino_2_bottom))-1):\n",
    "\n",
    "        if domino_2_top[index] ==  domino_2_bottom[index]:\n",
    "            match_list.append(1) ## 1 - dist (a,a) = 1\n",
    "        else:\n",
    "            match_list.append(0)  ## 1 - dist (a,b) = 0\n",
    "\n",
    "    match = min(match_list)\n",
    "\n",
    "    ##################Extend##################\n",
    "\n",
    "    # print(trajectory,trajectory1)\n",
    "    extend_list= list()\n",
    "\n",
    "    for index in range(min(len(trajectory), len(trajectory1))):\n",
    "\n",
    "        if trajectory[index] ==  trajectory1[index]:\n",
    "            extend_list.append(1) ## 1 - dist (a,a) = 1\n",
    "        else:\n",
    "            extend_list.append(0)  ## 1 - dist (a,b) = 0\n",
    "    \n",
    "    extend = min(extend_list)\n",
    "\n",
    "   \n",
    "\n",
    "    reward = max (-1*semimatch , min(extend, match))\n",
    "\n",
    "    return reward\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    env = PCPMDPEnv()\n",
    "    env1 = PCPMDPEnv()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    state = env.reset()\n",
    "    state1 = env1.reset()\n",
    "\n",
    "\n",
    "\n",
    "    done = False\n",
    "    done1 = False \n",
    "\n",
    "\n",
    "    total_reward = 0\n",
    "    total_reward1 = 0\n",
    "\n",
    "    \n",
    "\n",
    "    step = 0\n",
    "\n",
    "    act = [1,2,3,4,5]\n",
    "\n",
    "    while (done == False or done1 == False):\n",
    "        # action = np.random.choice(env.action_space)\n",
    "        # action1 = np.random.choice(env1.action_space)\n",
    "\n",
    "        action = 1\n",
    "        action1 = act[step]\n",
    "\n",
    "        if done == False:\n",
    "            \n",
    "            trajectory, domino = env.step(action)\n",
    "\n",
    "\n",
    "            # print(\"agent 0\")\n",
    "            # env.render()\n",
    "\n",
    "        if done1 == False:\n",
    "            trajectory1, domino1 = env1.step(action1)\n",
    "            \n",
    "\n",
    "        \n",
    "        reward(trajectory, trajectory1, domino, domino1, step)\n",
    "\n",
    "        time.sleep(1)\n",
    "            \n",
    "\n",
    "        step += 1\n",
    "\n",
    "\n",
    "    # print(f\"Reward agent 0: {total_reward}\")\n",
    "    # print(f\"Reward agent 1: {total_reward1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
